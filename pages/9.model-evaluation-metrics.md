# 9\. Model Evaluation & Metrics

# 

* * *

## Why Model Evaluation is Important?

# 

Model evaluation tells us:

*   How good the model is
    
*   How accurate predictions are
    
*   Whether the model is usable in real life
    

ðŸ“Œ **Interview line:**  
Model evaluation measures how well a machine learning model performs.

* * *

# A. Regression Metrics

# 

Used when output is a **number**.

* * *

## 1\. Mean Absolute Error (MAE)

### What is it?

# 

Average of absolute errors.

Formula:

MAE = |actual - predicted|

### Meaning

# 

*   Shows average mistake
    
*   Easy to understand
    

ðŸ“Œ Lower MAE = better model

ðŸ“Œ **Interview line:**  
MAE measures average absolute difference between actual and predicted values.

* * *

## 2\. Mean Squared Error (MSE)

### What is it?

# 

Average of **squared errors**.

Formula:

MSE = (actual - predicted)Â²

### Meaning

# 

*   Punishes large errors more
    
*   Sensitive to outliers
    

ðŸ“Œ Used in optimization

* * *

## 3\. Root Mean Squared Error (RMSE)

### What is it?

# 

Square root of MSE.

Formula:

RMSE = âˆšMSE

### Meaning

# 

*   Same unit as output
    
*   Easier to interpret than MSE
    

ðŸ“Œ **Interview tip:**  
RMSE penalizes large errors more than MAE.

* * *

## 4\. RÂ² Score (Coefficient of Determination)

### What is it?

# 

Explains how much variance is explained by the model.

Range:

0 to 1 (can be negative)

### Meaning

# 

*   1 â†’ perfect model
    
*   0 â†’ poor model
    

ðŸ“Œ **Interview line:**  
RÂ² shows how well independent variables explain the target variable.

* * *

# B. Classification Metrics

# 

Used when output is a **class**.

* * *

## Confusion Matrix

# 

A table showing:

*   True Positive (TP)
    
*   True Negative (TN)
    
*   False Positive (FP)
    
*   False Negative (FN)
    

### Example

# 

| Actual / Predicted | Yes | No |
| --- | --- | --- |
| Yes | TP | FN |
| No | FP | TN |

ðŸ“Œ Base for many metrics.

* * *

## 1\. Accuracy

### What is it?

# 

Overall correctness of model.

Formula:

Accuracy = (TP + TN) / Total

### Limitation

# 

*   Bad for imbalanced data
    

ðŸ“Œ **Interview line:**  
Accuracy measures total correct predictions.

* * *

## 2\. Precision

### What is it?

# 

Out of predicted positives, how many are correct?

Formula:

Precision = TP / (TP + FP)

### Used when:

# 

*   False positives are costly
    

ðŸ“Œ Example: Spam detection

* * *

## 3\. Recall (Sensitivity)

### What is it?

# 

Out of actual positives, how many are detected?

Formula:

Recall = TP / (TP + FN)

### Used when:

# 

*   Missing positive cases is dangerous
    

ðŸ“Œ Example: Disease detection

* * *

## 4\. F1-Score

### What is it?

# 

Balance between precision and recall.

Formula:

F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

ðŸ“Œ Best for imbalanced datasets.

* * *

## 5\. ROC-AUC

### ROC Curve

# 

*   Plots True Positive Rate vs False Positive Rate
    

### AUC

# 

*   Area under ROC curve
    

Range:

0 to 1

### Meaning

# 

*   Higher AUC = better model
    

ðŸ“Œ **Interview line:**  
ROC-AUC measures modelâ€™s ability to distinguish between classes.

* * *

## Which Metric to Use? (Interview Favorite)

# 

| Problem | Metric |
| --- | --- |
| Regression | RMSE, RÂ² |
| Balanced classification | Accuracy |
| Imbalanced data | Precision, Recall, F1 |
| Medical/Fraud | Recall |
| Spam | Precision |

* * *

## Quick Interview Summary

# 

âœ… Regression â†’ MAE, MSE, RMSE, RÂ²  
âœ… Classification â†’ Accuracy, Precision, Recall, F1  
âœ… Confusion matrix is base  
âœ… Accuracy fails on imbalanced data  
âœ… F1-score balances precision & recall