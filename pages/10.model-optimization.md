# 10\. Model Optimization

# 

* * *

## What is Model Optimization?

# 

Model optimization means **improving a modelâ€™s performance** so it works well on **new, unseen data**.

ðŸ“Œ **Interview line:**  
Model optimization improves model accuracy and generalization.

* * *

## 1\. Biasâ€“Variance Tradeoff

### Bias

# 

*   Error due to **simple model**
    
*   Model makes strong assumptions
    

Example:

*   Linear model for complex data
    

### Variance

# 

*   Error due to **complex model**
    
*   Model memorizes training data
    

ðŸ“Œ **Goal:**  
Low bias + low variance

ðŸ“Œ **Interview line:**  
Biasâ€“variance tradeoff balances underfitting and overfitting.

* * *

## 2\. Overfitting & Underfitting

### Underfitting

# 

*   Model is too simple
    
*   Poor training & testing performance
    

Example:

*   Straight line for curved data
    

### Overfitting

# 

*   Model is too complex
    
*   Good training, poor testing performance
    

Example:

*   Memorizing data
    

ðŸ“Œ **Interview line:**  
Overfitting occurs when a model learns noise instead of patterns.

* * *

## 3\. Cross-Validation

### What is it?

# 

Splits data into **multiple parts** to test model stability.

### K-Fold Cross-Validation

# 

*   Data divided into K parts
    
*   Train on K-1 parts
    
*   Test on remaining part
    
*   Repeat K times
    

ðŸ“Œ Gives reliable performance estimate.

ðŸ“Œ **Interview line:**  
Cross-validation helps evaluate model performance more accurately.

* * *

## 4\. Hyperparameter Tuning

### What are hyperparameters?

# 

*   Set **before training**
    
*   Control model behavior
    

### Examples

# 

*   Learning rate
    
*   Number of trees
    
*   Max depth
    

ðŸ“Œ Proper tuning improves accuracy.

* * *

## 5\. Grid Search

### What is it?

# 

*   Tries **all possible combinations** of hyperparameters
    

### Advantage

# 

*   Finds best parameters
    

### Disadvantage

# 

*   Very slow
    
*   Computationally expensive
    

ðŸ“Œ Used when parameter space is small.

* * *

## 6\. Random Search

### What is it?

# 

*   Tries **random combinations** of hyperparameters
    

### Advantage

# 

*   Faster than grid search
    
*   Works well for large parameter space
    

### Disadvantage

# 

*   Might miss best combination
    

ðŸ“Œ **Interview line:**  
Random search is faster and more efficient than grid search.

* * *

## Grid Search vs Random Search

# 

| Feature | Grid Search | Random Search |
| --- | --- | --- |
| Speed | Slow | Faster |
| Coverage | Full | Random |
| Use case | Small params | Large params |

* * *

## How to Reduce Overfitting (Interview Favorite)

# 

*   More training data
    
*   Regularization (L1, L2)
    
*   Cross-validation
    
*   Pruning trees
    
*   Early stopping
    

* * *

## Quick Interview Summary

# 

âœ… Optimize for generalization  
âœ… Biasâ€“variance tradeoff is key  
âœ… Overfitting is common problem  
âœ… Cross-validation gives reliable scores  
âœ… Hyperparameters improve model  
âœ… Random search is faster