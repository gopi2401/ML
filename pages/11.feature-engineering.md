# 11\. Feature Engineering

# 

* * *

## What is Feature Engineering?

# 

**Feature Engineering** is the process of **creating, selecting, and transforming features** to improve model performance.

ðŸ“Œ **Interview line:**  
Feature engineering is the process of selecting and transforming features to improve model accuracy.

* * *

## Why Feature Engineering is Important?

# 

*   ML models learn only from features
    
*   Better features â†’ better predictions
    
*   Reduces noise and overfitting
    
*   Improves training speed
    

* * *

## 1\. Feature Selection

### What is it?

# 

Choosing the **most important features** and removing unnecessary ones.

### Why?

# 

*   Reduces overfitting
    
*   Improves accuracy
    
*   Faster training
    

### Common Methods

# 

*   Filter methods (correlation)
    
*   Wrapper methods (RFE)
    
*   Embedded methods (Lasso, Tree-based)
    

ðŸ“Œ **Interview line:**  
Feature selection removes irrelevant and redundant features.

* * *

## 2\. Feature Extraction

### What is it?

# 

Creating **new features** from existing data.

### Examples

# 

*   Text â†’ TF-IDF features
    
*   Images â†’ pixel values
    
*   Dates â†’ day, month, year
    

ðŸ“Œ Used when raw data is not usable directly.

ðŸ“Œ **Interview line:**  
Feature extraction transforms raw data into meaningful features.

* * *

## 3\. Feature Importance

### What is it?

# 

Shows **how much each feature contributes** to the prediction.

### Methods

# 

*   Decision Trees
    
*   Random Forest
    
*   XGBoost
    

### Why useful?

# 

*   Model explainability
    
*   Feature selection
    
*   Debugging models
    

ðŸ“Œ **Interview line:**  
Feature importance helps understand which features affect predictions most.

* * *

## 4\. Dimensionality Reduction Techniques

### What is it?

# 

Reducing the **number of features** while keeping important information.

### Why?

# 

*   Avoid curse of dimensionality
    
*   Faster computation
    
*   Better visualization
    

* * *

### Common Techniques

#### 1\. PCA (Principal Component Analysis)

# 

*   Unsupervised
    
*   Keeps maximum variance
    

#### 2\. LDA (Linear Discriminant Analysis)

# 

*   Supervised
    
*   Maximizes class separation
    

#### 3\. Autoencoders

# 

*   Neural network based
    
*   Used in deep learning
    

ðŸ“Œ **Interview line:**  
Dimensionality reduction reduces feature space while preserving information.

* * *

## Feature Selection vs Feature Extraction

# 

| Feature | Selection | Extraction |
| --- | --- | --- |
| Removes features | Yes | No |
| Creates new features | No | Yes |
| Complexity | Low | High |

* * *

## Quick Interview Summary

# 

âœ… Feature engineering improves model quality  
âœ… Feature selection removes useless features  
âœ… Feature extraction creates new features  
âœ… Feature importance explains model  
âœ… Dimensionality reduction simplifies data