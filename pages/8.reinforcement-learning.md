# 8\. Reinforcement Learning (RL)

# 

* * *

## What is Reinforcement Learning?

# 

**Reinforcement Learning** is a type of Machine Learning where an **agent learns by interacting with an environment** and improves by **trial and error**.

ðŸ‘‰ The agent learns based on **rewards and penalties**.

ðŸ“Œ **Interview line:**  
Reinforcement learning trains an agent to make decisions by maximizing rewards.

* * *

## Key Components of Reinforcement Learning

# 

* * *

## 1\. Agent

# 

*   The **learner or decision maker**
    
*   Takes actions
    

### Example

# 

*   Robot
    
*   Game player
    
*   Self-driving car
    

* * *

## 2\. Environment

# 

*   The **world** where the agent operates
    
*   Responds to agent actions
    

### Example

# 

*   Game board
    
*   Road
    
*   Maze
    

* * *

## 3\. Reward

# 

*   Feedback given to agent
    
*   Positive â†’ good action
    
*   Negative â†’ bad action
    

### Example

# 

*   +10 points for winning
    
*   \-1 for wrong move
    

ðŸ“Œ Reward guides learning.

* * *

## RL Interaction Loop (Simple)

# 

Agent â†’ Action â†’ Environment â†’ Reward â†’ Agent

* * *

## Markov Decision Process (MDP)

# 

MDP is the **mathematical framework** of Reinforcement Learning.

### Components of MDP

# 

*   State (S)
    
*   Action (A)
    
*   Reward (R)
    
*   Transition probability
    
*   Discount factor (Î³)
    

### Simple Meaning

# 

> Future depends only on **current state**, not past history.

ðŸ“Œ **Interview line:**  
MDP models decision-making where outcomes depend on current state and action.

* * *

## Q-Learning

### What is it?

# 

A **value-based** Reinforcement Learning algorithm.

### Idea

# 

Learns the value of **stateâ€“action pairs**.

### Q-Table

# 

Q(state, action)

### Features

# 

*   Off-policy learning
    
*   Learns optimal policy independently
    

ðŸ“Œ Used in:

*   Games
    
*   Navigation problems
    

ðŸ“Œ **Interview line:**  
Q-Learning learns the best action by maximizing future rewards.

* * *

## SARSA

### What is it?

# 

Another value-based RL algorithm.

### Difference from Q-Learning

# 

*   SARSA is **on-policy**
    
*   Learns from actions actually taken
    

Formula order:

State â†’ Action â†’ Reward â†’ State â†’ Action

ðŸ“Œ Safer but slower.

* * *

## Q-Learning vs SARSA

# 

| Feature | Q-Learning | SARSA |
| --- | --- | --- |
| Policy | Off-policy | On-policy |
| Risk | Aggressive | Conservative |
| Learning | Faster | Safer |

* * *

## Policy-Based vs Value-Based Methods

# 

* * *

## Value-Based Methods

# 

*   Learn **value function**
    
*   Policy is derived from values
    

### Example

# 

*   Q-Learning
    
*   SARSA
    

ðŸ“Œ Choose best action based on highest value.

* * *

## Policy-Based Methods

# 

*   Learn **policy directly**
    
*   No value table
    

### Example

# 

*   Policy Gradient
    
*   REINFORCE
    

ðŸ“Œ Used in complex environments.

* * *

## Real-World Applications of RL

# 

*   Game playing (Chess, Go)
    
*   Robotics
    
*   Self-driving cars
    
*   Recommendation systems
    
*   Traffic control
    

* * *

## Quick Interview Summary

# 

âœ… RL learns by trial and error  
âœ… Agent interacts with environment  
âœ… Reward guides learning  
âœ… MDP is RL foundation  
âœ… Q-Learning = off-policy  
âœ… SARSA = on-policy