#   

# 6\. Supervised Learning

# 

* * *

## What is Supervised Learning?

# 

Supervised Learning is a type of ML where the model learns from **labeled data**.

ðŸ‘‰ Input data **with correct output** is given.

ðŸ“Œ **Interview line:**  
Supervised learning uses labeled data to train a model to predict outputs.

* * *

## Two Types of Supervised Learning

# 

1.  **Regression** â†’ Output is a **number**
    
2.  **Classification** â†’ Output is a **category**
    

* * *

# A. Regression

# 

Regression is used when the output is **continuous (numeric)**.

### Examples

# 

*   House price prediction
    
*   Salary prediction
    
*   Temperature prediction
    

* * *

## 1\. Linear Regression

### What is it?

# 

Finds a **straight-line relationship** between input and output.

Formula:

y = mx + c

### Example

# 

*   House size â†’ Price
    

ðŸ“Œ **Interview line:**  
Linear regression models the relationship between variables using a straight line.

* * *

## 2\. Multiple Linear Regression

### What is it?

# 

Uses **multiple input variables**.

Formula:

y = m1x1 + m2x2 + m3x3 + c

### Example

# 

*   Size + Location + Rooms â†’ Price
    

ðŸ“Œ Used when many features affect output.

* * *

## 3\. Polynomial Regression

### What is it?

# 

Used when data is **not a straight line**.

Example:

*   Curved relationship
    

ðŸ“Œ **Interview line:**  
Polynomial regression handles non-linear data.

* * *

## 4\. Ridge Regression (L2)

### Why?

# 

Reduces **overfitting**.

### How?

# 

Adds penalty for large coefficients.

ðŸ“Œ Used when features are correlated.

* * *

## 5\. Lasso Regression (L1)

### Why?

# 

Reduces overfitting and **removes useless features**.

ðŸ“Œ Performs feature selection.

* * *

## 6\. Elastic Net

# 

Combination of:

*   Ridge + Lasso
    

ðŸ“Œ Best when:

*   Many features
    
*   Multicollinearity exists
    

* * *

# B. Classification

# 

Classification is used when the output is a **category or class**.

### Examples

# 

*   Spam / Not Spam
    
*   Yes / No
    
*   Disease / No Disease
    

* * *

## 1\. Logistic Regression

### What is it?

# 

Used for **binary classification**.

Output:

0 or 1

Uses **sigmoid function**.

ðŸ“Œ **Interview line:**  
Logistic regression predicts probability for classification problems.

* * *

## 2\. K-Nearest Neighbors (KNN)

### How it works?

# 

*   Finds nearest K data points
    
*   Majority vote decides class
    

### Example

# 

*   Movie recommendation
    
*   Pattern recognition
    

ðŸ“Œ Simple but slow for large data.

* * *

## 3\. Naive Bayes

### Based on:

# 

Bayes Theorem

### Assumption:

# 

Features are independent.

### Used in:

# 

*   Spam detection
    
*   Text classification
    

ðŸ“Œ Very fast and efficient.

* * *

## 4\. Decision Tree

### How it works?

# 

Uses **if-else conditions**.

### Example

# 

Is age > 30?

 â”œ Yes â†’ Buy

 â”” No â†’ Don't Buy

ðŸ“Œ Easy to understand but can overfit.

* * *

## 5\. Random Forest

### What is it?

# 

Collection of many decision trees.

### Advantage

# 

*   High accuracy
    
*   Reduces overfitting
    

ðŸ“Œ **Interview line:**  
Random Forest is an ensemble of decision trees.

* * *

## 6\. Support Vector Machine (SVM)

### What is it?

# 

Finds the **best boundary** between classes.

### Uses

# 

*   High-dimensional data
    
*   Text classification
    

ðŸ“Œ Works well with small datasets.

* * *

## 7\. Gradient Boosting

### What is it?

# 

Builds models **step by step** to correct errors.

### Advantage

# 

*   Very accurate
    

ðŸ“Œ Slower but powerful.

* * *

## 8\. XGBoost

### What is it?

# 

Optimized version of Gradient Boosting.

### Features

# 

*   Fast
    
*   Handles missing data
    
*   High performance
    

ðŸ“Œ **Industry-level algorithm**

* * *

## Regression vs Classification (Quick Table)

# 

| Feature | Regression | Classification |
| --- | --- | --- |
| Output | Number | Category |
| Example | Price | Spam |
| Algorithms | Linear | Logistic |

* * *

## Quick Interview Summary

# 

âœ… Supervised learning uses labeled data  
âœ… Regression â†’ numeric output  
âœ… Classification â†’ categorical output  
âœ… Random Forest & XGBoost are powerful  
âœ… Overfitting is common problem