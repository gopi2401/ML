#   

# 13\. Deep Learning (Intro)

# 

* * *

## What is Deep Learning?

# 

**Deep Learning** is a **subset of Machine Learning** that uses **Neural Networks with many layers** to learn from data.

ğŸ“Œ **Simple definition:**  
Deep Learning teaches computers to learn like the human brain using neural networks.

* * *

## ML vs Deep Learning (Quick)

# 

| Machine Learning | Deep Learning |
| --- | --- |
| Needs manual features | Learns features automatically |
| Works on small data | Needs large data |
| Faster training | Slower training |
| Less complex | Very complex |

* * *

## Neural Network Basics

# 

A **Neural Network** is made of:

1.  **Input layer**
    
2.  **Hidden layer(s)**
    
3.  **Output layer**
    

Each layer has **neurons**.

ğŸ“Œ **Neuron = small calculation unit**

* * *

## How a Neuron Works

# 

1.  Takes input
    
2.  Multiplies by weights
    
3.  Adds bias
    
4.  Applies activation function
    
5.  Gives output
    

* * *

## Perceptron

### What is a Perceptron?

# 

*   Simplest neural network
    
*   Single neuron
    
*   Used for binary classification
    

### Formula (conceptual)

# 

Output = Activation(w1x1 + w2x2 + bias)

ğŸ“Œ **Interview line:**  
A perceptron is the basic unit of a neural network.

* * *

## Activation Functions

### Why needed?

# 

*   Add **non-linearity**
    
*   Without them, network cannot learn complex patterns
    

* * *

### Common Activation Functions

#### 1\. Sigmoid

# 

*   Output: 0 to 1
    
*   Used in binary classification
    

âŒ Problem: Vanishing gradient

* * *

#### 2\. ReLU (Most popular)

# 

*   Output: max(0, x)
    

âœ… Fast  
âŒ Dying ReLU problem

* * *

#### 3\. Tanh

# 

*   Output: -1 to 1
    
*   Better than sigmoid
    

* * *

#### 4\. Softmax

# 

*   Used in multi-class classification
    
*   Gives probability for each class
    

* * *

## Backpropagation

### What is Backpropagation?

# 

*   Method to **update weights**
    
*   Error moves **backwards**
    
*   Uses gradient descent
    

ğŸ“Œ **Simple meaning:**  
Backpropagation teaches the network how wrong it is and fixes weights.

* * *

## Loss Functions

### What is Loss Function?

# 

*   Measures **error**
    
*   Lower loss = better model
    

* * *

### Common Loss Functions

# 

| Problem | Loss Function |
| --- | --- |
| Regression | Mean Squared Error |
| Binary classification | Binary Cross-Entropy |
| Multi-class classification | Categorical Cross-Entropy |

ğŸ“Œ **Interview line:**  
Loss function tells how far prediction is from actual value.

* * *

## Optimizers

# 

Optimizers update weights to **reduce loss**.

* * *

### 1\. SGD (Stochastic Gradient Descent)

# 

*   Updates weights step by step
    
*   Simple but slow
    

ğŸ“Œ Can get stuck in local minima

* * *

### 2\. Adam (Most used)

# 

*   Adaptive learning rate
    
*   Fast and efficient
    

ğŸ“Œ **Interview favorite:**  
Adam is widely used because it is fast and adaptive.

* * *

## Training Process (Simple Flow)

# 

1.  Forward propagation
    
2.  Calculate loss
    
3.  Backpropagation
    
4.  Update weights
    
5.  Repeat until loss is low
    

* * *

## Deep Learning Applications

# 

*   Image recognition
    
*   Face detection
    
*   Speech recognition
    
*   Chatbots
    
*   Self-driving cars
    

* * *

## Interview Quick Notes ğŸ“

# 

âœ… Deep Learning = Neural Networks with many layers  
âœ… Perceptron = basic unit  
âœ… ReLU is most popular activation  
âœ… Backpropagation updates weights  
âœ… Adam is best optimizer  
âœ… Softmax for multi-class output