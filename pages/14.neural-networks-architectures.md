# 14\. Neural Networks & Architectures

# 

* * *

## What is a Neural Network Architecture?

# 

A **neural network architecture** is the **structure** of the network:

*   Number of layers
    
*   Type of layers
    
*   How data flows
    

Different problems need **different architectures**.

* * *

## 1\. Artificial Neural Networks (ANN)

### What is ANN?

# 

*   Basic form of neural network
    
*   Fully connected layers
    
*   Data flows in **one direction**
    

ðŸ“Œ Also called **Feed Forward Neural Network**

* * *

### Structure

# 

Input â†’ Hidden layer(s) â†’ Output

* * *

### Used For

# 

*   Tabular data
    
*   Regression
    
*   Classification
    

* * *

### Advantages

# 

*   Simple
    
*   Easy to implement
    

### Disadvantages

# 

*   Not good for images or sequences
    

ðŸ“Œ **Interview line:**  
ANN is a fully connected feed-forward neural network.

* * *

## 2\. Convolutional Neural Networks (CNN)

### What is CNN?

# 

*   Designed for **image data**
    
*   Uses **convolution layers**
    

ðŸ“Œ Learns spatial features automatically.

* * *

### Key Layers in CNN

#### 1\. Convolution Layer

# 

*   Applies filters
    
*   Detects edges, shapes
    

#### 2\. Pooling Layer

# 

*   Reduces image size
    
*   Max Pooling is common
    

#### 3\. Fully Connected Layer

# 

*   Final prediction
    

* * *

### Used For

# 

*   Image classification
    
*   Face recognition
    
*   Object detection
    

ðŸ“Œ **Interview line:**  
CNN is mainly used for image processing tasks.

* * *

## 3\. Recurrent Neural Networks (RNN)

### What is RNN?

# 

*   Designed for **sequence data**
    
*   Has **memory**
    
*   Output depends on previous input
    

ðŸ“Œ Example: text, speech, time-series

* * *

### Problems with RNN

# 

*   Vanishing gradient
    
*   Cannot remember long sequences
    

ðŸ“Œ **Interview line:**  
RNN keeps memory of previous inputs.

* * *

## 4\. LSTM (Long Short-Term Memory)

### What is LSTM?

# 

*   Special type of RNN
    
*   Solves vanishing gradient problem
    
*   Can remember long-term data
    

* * *

### Key Components (No need deep math)

# 

*   Forget gate
    
*   Input gate
    
*   Output gate
    

* * *

### Used For

# 

*   Speech recognition
    
*   Machine translation
    
*   Time series forecasting
    

ðŸ“Œ **Interview line:**  
LSTM is an improved RNN that remembers long-term dependencies.

* * *

## 5\. GRU (Gated Recurrent Unit)

### What is GRU?

# 

*   Simplified version of LSTM
    
*   Fewer gates
    
*   Faster training
    

* * *

### LSTM vs GRU

# 

| Feature | LSTM | GRU |
| --- | --- | --- |
| Gates | 3 | 2 |
| Speed | Slower | Faster |
| Complexity | High | Medium |

ðŸ“Œ **Interview tip:**  
Use GRU when dataset is small and speed matters.

* * *

## 6\. Autoencoders

### What is an Autoencoder?

# 

*   Unsupervised neural network
    
*   Learns to **compress and reconstruct** data
    

* * *

### Structure

# 

Input â†’ Encoder â†’ Latent space â†’ Decoder â†’ Output

* * *

### Used For

# 

*   Dimensionality reduction
    
*   Anomaly detection
    
*   Noise removal
    

ðŸ“Œ **Interview line:**  
Autoencoders learn compressed representations of data.

* * *

## Architecture Summary Table

# 

| Architecture | Best For |
| --- | --- |
| ANN | Tabular data |
| CNN | Images |
| RNN | Sequential data |
| LSTM | Long sequences |
| GRU | Fast sequence learning |
| Autoencoder | Compression & anomalies |

* * *

## Interview Quick Notes ðŸ§ 

# 

âœ… ANN = basic neural network  
âœ… CNN = image data  
âœ… RNN = sequence + memory  
âœ… LSTM solves vanishing gradient  
âœ… GRU is faster than LSTM  
âœ… Autoencoder is unsupervised