12. Ensemble Learning
What is Ensemble Learning?

Ensemble Learning combines multiple models to create a stronger and more accurate model.

ðŸ‘‰ Idea: Many weak models together = strong model

ðŸ“Œ Interview line:
Ensemble learning improves accuracy by combining multiple models.

Why Ensemble Learning Works?

Reduces overfitting

Improves accuracy

More stable predictions

Types of Ensemble Learning

Bagging

Boosting

Stacking

1. Bagging (Bootstrap Aggregating)
What is it?

Train multiple models independently

Use different random samples

Combine results (average or voting)

Example

Random Forest

Advantage

Reduces variance

Handles overfitting

ðŸ“Œ Interview line:
Bagging reduces variance by training models on different data samples.

2. Boosting
What is it?

Models are trained sequentially

Each new model fixes errors of the previous one

Advantage

High accuracy

Focuses on difficult data points

ðŸ“Œ Interview line:
Boosting improves performance by focusing on previous model errors.

3. Stacking
What is it?

Combine different types of models

Use a meta-model to make final prediction

Example

Logistic Regression + Tree + SVM â†’ Meta model

ðŸ“Œ Very powerful but complex.

Popular Ensemble Algorithms
4. Random Forest
What is it?

Ensemble of Decision Trees

Uses Bagging

Features

Random data sampling

Random feature selection

Advantages

High accuracy

Handles missing values

Reduces overfitting

ðŸ“Œ Interview line:
Random Forest is an ensemble of decision trees using bagging.

5. Gradient Boosting
What is it?

Boosting-based algorithm

Builds models step by step

Minimizes loss function

Advantage

Very accurate

Disadvantage

Slow

Sensitive to overfitting

ðŸ“Œ Used in competitions.

6. AdaBoost (Adaptive Boosting)
What is it?

First boosting algorithm

Increases weight of misclassified points

Advantage

Simple

Effective for small datasets

Disadvantage

Sensitive to noise

ðŸ“Œ Interview line:
AdaBoost focuses more on wrongly predicted samples.

Bagging vs Boosting (Interview Table)
Feature	Bagging	Boosting
Training	Parallel	Sequential
Focus	Reduce variance	Reduce bias
Overfitting	Less	Possible
Example	Random Forest	AdaBoost
Quick Interview Summary

âœ… Ensemble = multiple models
âœ… Bagging reduces variance
âœ… Boosting reduces bias
âœ… Random Forest is bagging-based
âœ… Gradient Boosting is powerful
âœ… AdaBoost adapts to errors