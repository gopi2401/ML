# 7\. Unsupervised Learning

# 

* * *

## What is Unsupervised Learning?

# 

Unsupervised Learning is a type of ML where **no labeled data** is given.

ğŸ‘‰ The model **finds patterns by itself**.

ğŸ“Œ **Interview line:**  
Unsupervised learning discovers hidden patterns in unlabeled data.

* * *

## Main Tasks in Unsupervised Learning

# 

1.  **Clustering**
    
2.  **Dimensionality Reduction**
    
3.  **Association Rules**
    

* * *

# A. Clustering

# 

Clustering means **grouping similar data points together**.

### Example

# 

*   Group customers based on buying behavior
    
*   Group students by marks
    

* * *

## 1\. K-Means Clustering

### How it works (simple):

# 

1.  Choose K (number of clusters)
    
2.  Place K random centroids
    
3.  Assign points to nearest centroid
    
4.  Update centroids
    
5.  Repeat until stable
    

### Example

# 

*   Customer segmentation
    

### Pros

# 

*   Simple
    
*   Fast
    

### Cons

# 

*   Must choose K
    
*   Sensitive to outliers
    

ğŸ“Œ **Interview line:**  
K-Means groups data into K clusters based on distance.

* * *

## 2\. Hierarchical Clustering

### What is it?

# 

Creates a **tree-like structure** (dendrogram).

### Types

# 

*   Agglomerative (bottom-up)
    
*   Divisive (top-down)
    

### Advantage

# 

*   No need to choose K first
    

### Disadvantage

# 

*   Slow for large datasets
    

ğŸ“Œ Used in biology and research.

* * *

## 3\. DBSCAN

### What is it?

# 

Density-Based clustering.

### Groups points based on density.

### Advantages

# 

*   Finds clusters of any shape
    
*   Handles noise well
    
*   No need to choose K
    

### Disadvantage

# 

*   Struggles with varying density
    

ğŸ“Œ **Interview line:**  
DBSCAN is good for noisy and irregular data.

* * *

# B. Dimensionality Reduction

# 

Used to **reduce number of features** while keeping important information.

### Why needed?

# 

*   High dimensional data is complex
    
*   Faster training
    
*   Better visualization
    

* * *

## 1\. PCA (Principal Component Analysis)

### What is it?

# 

Transforms data into **new variables (principal components)**.

### Key idea

# 

*   Maximum variance
    
*   Minimum information loss
    

### Uses

# 

*   Feature reduction
    
*   Noise removal
    

ğŸ“Œ **Interview line:**  
PCA reduces dimensions by keeping maximum variance.

* * *

## 2\. LDA (Linear Discriminant Analysis)

### What is it?

# 

Dimension reduction **with class labels**.

âš ï¸ Note:  
LDA is **supervised**, but often taught here.

### Goal

# 

*   Maximize class separation
    

ğŸ“Œ Used in classification tasks.

* * *

## 3\. t-SNE

### What is it?

# 

Used for **visualizing high-dimensional data** in 2D or 3D.

### Example

# 

*   Visualizing word embeddings
    
*   Image features
    

### Limitation

# 

*   Slow
    
*   Not for large datasets
    

ğŸ“Œ Best for visualization, not modeling.

* * *

# C. Association Rules

# 

Used to find **relationships between items**.

### Example

# 

If customer buys bread â†’ also buys butter

Used in:

*   Market basket analysis
    
*   Recommendation systems
    

* * *

## 1\. Apriori Algorithm

### How it works?

# 

*   Finds frequent item sets
    
*   Generates rules
    

### Measures

# 

*   Support
    
*   Confidence
    
*   Lift
    

ğŸ“Œ Slow for large datasets.

* * *

## 2\. FP-Growth

### What is it?

# 

Optimized version of Apriori.

### Advantages

# 

*   Faster
    
*   Uses tree structure
    
*   No candidate generation
    

ğŸ“Œ Preferred in real systems.

* * *

## Quick Interview Summary

# 

âœ… Unsupervised = no labels  
âœ… Clustering groups data  
âœ… K-Means is distance-based  
âœ… DBSCAN handles noise  
âœ… PCA reduces features  
âœ… Apriori finds item relationships